{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2b0df3e-ce80-473e-8b34-8e028af76541",
   "metadata": {},
   "source": [
    "## Models and data for the first Manuscript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f04720-da1c-4994-9b73-43e0b0fc9a81",
   "metadata": {},
   "source": [
    "### Import from the python files data_loader and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aa766e-2456-4cf5-baf4-a8ecbf88bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed910e0-bbfd-4520-8fcc-a127b6febb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import glorot_normal, glorot_uniform, he_normal, he_uniform, Constant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef9cba6-2ad4-46b6-a138-430f19845a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seeds():\n",
    "    \"\"\"\n",
    "        Sets seeds for reproducibility purposes\n",
    "    \"\"\"\n",
    "    np.random.seed(9)\n",
    "    tf.random.set_seed(12)\n",
    "    os.environ['PYTHONHASHSEED']=str(15)\n",
    "    print('Reset Seeds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179d16b2-8786-46cd-bfa7-58c34ea6bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae0b69f-57a4-493f-9e28-16db6754c50b",
   "metadata": {},
   "source": [
    "### Set the paths for the token, noonan, and non noonan files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac6fb4f-7889-46aa-a9d2-f715f11af6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_path = '../tokenizer/token_all.json'\n",
    "path_non_noonan = \"../data/combined_non_noonan.csv\"\n",
    "path_noonan = \"../data/noonan_r3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae08d0-3858-467a-b42c-f996fe9da993",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_array, attribute_names, n_string_list, n_patient_list = sort_data(path_noonan)\n",
    "nn_array, attribute_names, nn_string_list, nn_patient_list = sort_data(path_non_noonan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d787a3-7fd1-402e-b964-13aec4a9c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_int_list, indx_words, word_indx, token_path = string_to_ints(n_string_list, token_path)\n",
    "nn_int_list, indx_words, word_indx, token_path = string_to_ints(nn_string_list, token_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b596295-438e-428e-be65-8b1e0317adf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translate_to_string(indx_words, original):\n",
    "    translated = \"\"\n",
    "    for num in original:\n",
    "        translated += indx_words[num] + \" \"\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b192495-20e3-4e5f-bbad-1b6cbcd9d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_filter(int_list, length, lineend_index):\n",
    "    \"\"\"\n",
    "    Takes a list of list of ints and checks their lengths after removing the lineend term. Returns the indices of lists\n",
    "    of greater or equal length.\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    for index, item in enumerate(int_list):\n",
    "        no_lineend = item.copy()\n",
    "        no_lineend.remove(lineend_index)\n",
    "        if len(no_lineend) >= length:\n",
    "            filtered.append(index)\n",
    "    print(\"original count: {len1}\\t filtered count: {len2}\".format(len1=len(int_list), len2=len(filtered)))\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e118295a-e310-4a07-96f5-9d538a4b3ec1",
   "metadata": {},
   "source": [
    "### Do the filtering for length, minimum is 10 words (not including gender or lineend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbaf23c-feca-46f7-8207-1c70b4f4ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "lineend_index = word_indx[\"lineend\"]\n",
    "n_filtered_index = length_filter(n_int_list, 11, lineend_index)\n",
    "nn_filtered_index = length_filter(nn_int_list, 11, lineend_index)\n",
    "n_features = np.array(n_int_list, dtype=object)[n_filtered_index]\n",
    "nn_features = np.array(nn_int_list, dtype=object)[nn_filtered_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b02f96a-3c1c-420a-af74-767bb6e666df",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_patient_array = np.array(n_patient_list)[n_filtered_index]\n",
    "nn_patient_array = np.array(nn_patient_list)[nn_filtered_index]\n",
    "print(n_patient_array.shape)\n",
    "print(nn_patient_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efb042d-ff4c-4df0-aebd-d90a0a425136",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(n_features.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "n_features = n_features[indices]\n",
    "n_patient_array = n_patient_array[indices]\n",
    "\n",
    "indices = np.arange(nn_features.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "nn_features = nn_features[indices]\n",
    "nn_patient_array = nn_patient_array[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee854574-1c7d-4a1b-9adf-cbf4423543df",
   "metadata": {},
   "source": [
    "### Split data into k fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd88f5c-1907-449e-bc4c-45adfe39eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(indx_words)+ 1\n",
    "print(\"the shapes are %s for noonan and %s for non noonan\" %(n_features.shape[0], nn_features.shape[0]))\n",
    "\n",
    "k = 7\n",
    "k_fold_noonan = k_fold(k, n_features.shape[0])\n",
    "k_fold_non_noonan = k_fold(k, nn_features.shape[0])\n",
    "cross_validation = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1753d8-a74b-4c0d-8822-09b7ad8e1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "embedding_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e48484-c132-41b9-8de6-4f991b8107c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 9\n",
    "g_norm = glorot_normal(seed = seed)\n",
    "g_unif = glorot_uniform(seed = seed)\n",
    "he_norm = he_normal(seed = seed)\n",
    "he_unif = he_uniform(seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d4f51c-4ad1-48eb-9d78-225aa1e7658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inits = {\n",
    "#          'ReLU-glorot_normal': ('relu', g_norm),\n",
    "#          'ReLU-glorot_uniform': ('relu', g_unif),\n",
    "#          'ReLU-he_normal': ('relu', he_norm),\n",
    "#          'ReLU-he_uniform': ('relu', he_unif),\n",
    "#          'PReLU-glorot_normal': ('prelu', g_norm),\n",
    "#          'PReLU-glorot_uniform': ('prelu', g_unif),\n",
    "#          'PReLU-he_normal': ('prelu', he_norm),\n",
    "#          'PReLU-he_uniform': ('prelu', he_unif)\n",
    "#          }\n",
    "# init = inits['ReLU-he_uniform'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad2f0b6-5e8c-4319-a8d5-dfce41c7ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(MAX_SEQUENCE_LENGTH, num_words, embedding_size, model_type):\n",
    "    keras.backend.clear_session()\n",
    "    print(\"making model\")\n",
    "    print(\"numwords is %s training length is all\" %(num_words))\n",
    "\n",
    "    # shared layers\n",
    "    sequence_input = keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences = keras.layers.Embedding(num_words, embedding_size, input_length=MAX_SEQUENCE_LENGTH)(sequence_input)\n",
    "    \n",
    "    if model_type == 'conv':\n",
    "        l_conv = Conv1D(128, 3, activation='relu')(embedded_sequences)\n",
    "        l_pool = keras.layers.GlobalMaxPooling1D()(l_conv)\n",
    "        dense1 = keras.layers.Dense(64, activation='relu')(l_pool)\n",
    "        drop_1 = keras.layers.Dropout(0.5)(dense1)\n",
    "        dense1 = keras.layers.Dense(16, activation='relu')(drop_1)\n",
    "        drop_1 = keras.layers.Dropout(0.5)(dense1)\n",
    "\n",
    "    if model_type == 'lstm':\n",
    "        l_lstm = keras.layers.LSTM(64, return_sequences=True, dropout=0.1)(embedded_sequences)\n",
    "        l_pool = keras.layers.GlobalMaxPooling1D()(l_lstm)\n",
    "        dense1 = keras.layers.Dense(32, activation='relu')(l_pool)\n",
    "        drop_1 = keras.layers.Dropout(0.5)(dense1)\n",
    "\n",
    "    if model_type == 'bigru':\n",
    "        l_bigru = keras.layers.Bidirectional(keras.layers.GRU(64, return_sequences=True, dropout=0.1))(embedded_sequences)\n",
    "        l_pool = keras.layers.GlobalMaxPooling1D()(l_bigru)\n",
    "        dense1 = keras.layers.Dense(32, activation='relu')(l_pool)\n",
    "        drop_1 = keras.layers.Dropout(0.5)(dense1)\n",
    "        \n",
    "    if model_type == 'gru':\n",
    "        l_gru = keras.layers.GRU(128, return_sequences=True, dropout=0.1)(embedded_sequences)\n",
    "        l_pool = keras.layers.GlobalMaxPooling1D()(l_gru)\n",
    "        dense1 = keras.layers.Dense(32, activation='relu')(l_pool)\n",
    "        drop_1 = keras.layers.Dropout(0.5)(dense1)\n",
    "        \n",
    "    if model_type == 'dense':\n",
    "        flatten = keras.layers.Flatten()(embedded_sequences)\n",
    "        dense1 = keras.layers.Dense(16, activation='relu')(flatten)\n",
    "        drop_1 = keras.layers.Dropout(0.2)(dense1)\n",
    "    \n",
    "    preds_1 = keras.layers.Dense(1, activation='sigmoid')(drop_1)\n",
    "\n",
    "    model = keras.Model(sequence_input, outputs=preds_1)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC(curve=\"PR\")])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeb0363-0b02-436b-ab6c-6c81b79d218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_three(features, labels, patients):\n",
    "    \"\"\"\n",
    "    Takes an array of features, an array of labels, and patients and shuffles the rows\n",
    "    Args:\n",
    "        features - (np.ndarray) features array\n",
    "        labels - (np.ndarray) labels corresponding to the features\n",
    "        patients - (np.ndarray) patient id corresponding to the features\n",
    "    Returns:\n",
    "        features - (np.ndarray) features shuffled\n",
    "        labels - (np.ndarray) labels shuffled according to the features\n",
    "        patients - (np.ndarray) patient id shuffled according to the features\n",
    "    \"\"\"\n",
    "    indices = np.arange(features.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    return features[indices], labels[indices], patients[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8648bbe9-0a7a-42dd-9ebc-7b7806c4f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class savePredict(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback to save predictions after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self, validation_generator, validation_labels, validation_patients, output_path, pic_path, save_plot=False, save_pr_recall=False):\n",
    "        self.out = []\n",
    "        self.output_path = output_path\n",
    "        self.validation_generator = validation_generator\n",
    "        self.validation_labels = validation_labels\n",
    "        self.validation_patients = validation_patients\n",
    "        self.pic_path = pic_path\n",
    "        self.save_plot = save_plot\n",
    "        self.save_pr_recall = save_pr_recall\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        output = self.model.predict(self.validation_generator, verbose=2)\n",
    "        if self.save_pr_recall:\n",
    "            analysis = calculate_pr_recall(output, self.validation_labels, threshold=.01)\n",
    "            analysisdir = \"{output_path}_{epochNum}_pr.csv\".format(output_path=self.output_path, epochNum=epoch)\n",
    "            save_chart(analysis, analysisdir)\n",
    "\n",
    "        valdir = \"{output_path}_{epochNum}_val.csv\".format(output_path=self.output_path, epochNum=epoch)\n",
    "        with open(valdir, 'w', newline='\\n', encoding=\"ISO-8859-1\") as csvfile:\n",
    "            record_writer = csv.writer(csvfile, delimiter=',')\n",
    "            attribute_names = ['prediction', 'actual', 'patient']\n",
    "            record_writer.writerow(attribute_names)\n",
    "            for i  in range(len(output)):\n",
    "                row = [output[i][0], self.validation_labels[i], self.validation_patients[i]]\n",
    "                record_writer.writerow(row)\n",
    "\n",
    "        if self.save_plot:\n",
    "            chartdir = \"{chartdir}_{epochNum}_pr.pdf\".format(chartdir=self.pic_path, epochNum=epoch)\n",
    "            plot_pr_recall(valdir, chartdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4741d64-bb69-4cde-ab80-9ded1e29b257",
   "metadata": {},
   "source": [
    "### Resampling and creating static folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ab7a0a-0ec0-44db-8ca4-1654c2a436f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_k = []\n",
    "for i in range(k): \n",
    "    n_features_train_index = k_fold_noonan[0][i]\n",
    "    nn_features_train_index = k_fold_non_noonan[0][i]\n",
    "    n_features_validation_index = k_fold_noonan[1][i]\n",
    "    nn_features_validation_index = k_fold_non_noonan[1][i]\n",
    "    n_features_test_index = k_fold_noonan[2][i]\n",
    "    nn_features_test_index = k_fold_non_noonan[2][i]\n",
    "    \n",
    "    np.random.shuffle(n_features_train_index)\n",
    "    num_controls = 100 * len(n_features_train_index)\n",
    "    train_controls_index = nn_features_train_index[0:num_controls]\n",
    "    x_train_index = train_controls_index + n_features_train_index\n",
    "    y_train = np.array([0] * len(train_controls_index) + [1] * len(n_features_train_index))\n",
    "    x_train =  np.concatenate((nn_features[train_controls_index], n_features[n_features_train_index]), axis=None)\n",
    "    train_patients = np.concatenate((nn_patient_array[train_controls_index], n_patient_array[n_features_train_index]), axis=None)\n",
    "\n",
    "    np.random.shuffle(n_features_validation_index)\n",
    "    num_controls = 1000 * len(n_features_validation_index)\n",
    "    validation_controls_index = nn_features_validation_index[0:num_controls]\n",
    "    x_validation_index = validation_controls_index + n_features_validation_index\n",
    "    y_validation = np.array([0] * len(validation_controls_index) + [1] * len(n_features_validation_index))\n",
    "    x_validation = np.concatenate((nn_features[validation_controls_index], n_features[n_features_validation_index]), axis=None)\n",
    "    validation_patients = np.concatenate((nn_patient_array[validation_controls_index], n_patient_array[n_features_validation_index]), axis=None)\n",
    "\n",
    "    np.random.shuffle(n_features_test_index)\n",
    "    num_controls = 1000 * len(n_features_test_index)\n",
    "    test_controls_index = nn_features_test_index[0:num_controls]\n",
    "    x_test_index = test_controls_index + n_features_test_index\n",
    "    y_test = np.array([0] * len(test_controls_index) + [1] * len(n_features_test_index))\n",
    "    x_test = np.concatenate((nn_features[test_controls_index], n_features[n_features_test_index]), axis=None)\n",
    "    test_patients = np.concatenate((nn_patient_array[test_controls_index], n_patient_array[n_features_test_index]), axis=None)\n",
    "    \n",
    "    x_train = pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "    x_validation = pad_sequences(x_validation, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "    x_test = pad_sequences(x_test, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "    \n",
    "    x_train, y_train, train_patients = shuffle_three(x_train, y_train, train_patients)\n",
    "    x_validation, y_validation, validation_patients = shuffle_three(x_validation, y_validation, validation_patients)\n",
    "    x_test, y_test, test_patients = shuffle_three(x_test, y_test, test_patients)\n",
    "    resampled_k.append([[x_train, y_train, train_patients], [x_validation, y_validation, validation_patients], [x_test, y_test, test_patients]])\n",
    "    \n",
    "    print(\"shapes for data sets \\n train: {train}, \\t valiation: {validation}, \\t test: {test}\".format(train=x_train.shape[0], test=x_test.shape[0], validation=x_validation.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e926b6fc-921f-4e75-84de-b02cb2122f3a",
   "metadata": {},
   "source": [
    "### This is just to extract the list of names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7171182d-f3b6-406e-b64e-0436acad0af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_train, final_validation, final_test = resampled_k[-1]\n",
    "# train_df = pd.DataFrame(list(zip(final_train[2], final_train[1])), \n",
    "#                           columns=['patient_id', 'label'])\n",
    "# train_df_non_noonan = train_df[train_df.label != 1]\n",
    "# train_df_non_noonan.to_csv(index=False, path_or_buf=\"../models/20210908-160537_conv_gender/final_train_non_noonan.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93b1064-d400-496d-9a0a-09b48932b2f7",
   "metadata": {},
   "source": [
    "### To perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38e225-15b9-48cc-8311-0d70ed7c1125",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = ['conv', 'dense', 'bigru', 'gru', 'lstm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f066c83-9536-4dc8-af42-b8cc378bfbf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_type in model_types:\n",
    "\n",
    "    file_name = model_type + '_gender_final'\n",
    "    reset_seeds()\n",
    "\n",
    "    logdir = \"../logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_\" + file_name\n",
    "    modeldir = '../models/' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_\" + file_name\n",
    "    chartdir = '../pics/' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_\" + file_name\n",
    "    os.mkdir(logdir)\n",
    "    os.mkdir(modeldir)\n",
    "    os.mkdir(chartdir)\n",
    "\n",
    "    for i in range(k): \n",
    "        \"\"\"\n",
    "        first: get the training validation and test sets\n",
    "        \"\"\"\n",
    "        train, validation, test = resampled_k[i]\n",
    "\n",
    "        x_train = train[0]\n",
    "        y_train = train[1]\n",
    "        patient_train = train[2]\n",
    "        x_validation = validation[0]\n",
    "        y_validation = validation[1]\n",
    "        patient_validation = validation[2]\n",
    "        x_test = test[0]\n",
    "        y_test = test[1]\n",
    "        patient_test = test[2]\n",
    "\n",
    "        \"\"\"\n",
    "        second: make the actual model\n",
    "        \"\"\"\n",
    "        model = make_model(MAX_SEQUENCE_LENGTH, num_words, embedding_size, model_type)\n",
    "        display(keras.utils.plot_model(model, show_shapes=True)) \n",
    "        \"\"\"\n",
    "        third: make callbacks\n",
    "        \"\"\"\n",
    "        logpath = '{logdir}/fold_{foldNum}'.format(logdir=logdir, foldNum=i)\n",
    "        modelpath = '{modeldir}/fold_{foldNum}.h5'.format(modeldir=modeldir, foldNum=i)\n",
    "        picspath = '{chartdir}/fold_{foldNum}'.format(chartdir=chartdir, foldNum=i)\n",
    "        outpath = '{modeldir}/fold_{foldNum}'.format(modeldir=modeldir, foldNum=i)\n",
    "\n",
    "        tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logpath)\n",
    "\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(modelpath, save_best_only=False,\n",
    "            save_weights_only=False, monitor='val_auc', mode='max'),\n",
    "            tensorboard_callback,\n",
    "            savePredict(x_validation, y_validation, patient_validation, outpath, picspath)]\n",
    "\n",
    "        \"\"\"\n",
    "        fourth: fit the data\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"beginning training\")\n",
    "\n",
    "        if i == (k - 1):\n",
    "            mean_aucs = np.array(cross_validation).mean(axis=0)\n",
    "            max_aucs = max(mean_aucs)\n",
    "            max_auc_index = np.argmax(mean_aucs) + 1\n",
    "            print(\"max_auc is {max} at epoch {epoch}\".format(max=max_aucs, epoch=max_auc_index))\n",
    "\n",
    "            modelpath = '{modeldir}/fold_{foldNum}_final.h5'.format(modeldir=modeldir, foldNum=i)\n",
    "            picspath = '{chartdir}/fold_{foldNum}_final'.format(chartdir=chartdir, foldNum=i)\n",
    "            outpath = '{modeldir}/fold_{foldNum}_final'.format(modeldir=modeldir, foldNum=i)\n",
    "\n",
    "            callbacks = [\n",
    "                ModelCheckpoint(modelpath, save_best_only=False,\n",
    "                save_weights_only=False, monitor='val_auc', mode='max'),\n",
    "                tensorboard_callback,\n",
    "                savePredict(x_test, y_test, patient_test, outpath, picspath, save_plot=True, save_pr_recall=True)]\n",
    "            history = model.fit(x_train, y_train, validation_data=(x_test, y_test),\n",
    "                epochs=max_auc_index, callbacks=callbacks, verbose=1, batch_size=200)\n",
    "\n",
    "            \"\"\"\n",
    "            fifth: plot the cross_validation\n",
    "            \"\"\"\n",
    "            strlist = listdir(modeldir)\n",
    "            p = re.compile('fold_[0-9]*_{best_epoch}_val\\.csv'.format(best_epoch=max_auc_index - 1))\n",
    "            newlist = list(filter(p.match, strlist))\n",
    "            best_epoch_dir = modeldir + \"/best_epoch\"\n",
    "            os.mkdir(best_epoch_dir)\n",
    "            for file in newlist:\n",
    "                source = modeldir + '/' + file\n",
    "                dest = best_epoch_dir + '/' + file\n",
    "                copyfile(source, dest)\n",
    "            final_plot = \"{chartdir}/cross_validation_epoch_{best_epoch}.pdf\".format(chartdir=chartdir, best_epoch=max_auc_index)\n",
    "            plot_combined(best_epoch_dir, final_plot)\n",
    "\n",
    "        else:\n",
    "            history = model.fit(x_train, y_train, validation_data=(x_validation, y_validation),\n",
    "                epochs=20, callbacks=callbacks, verbose=1, batch_size=200)\n",
    "            print(history.history)\n",
    "            cross_validation.append(history.history['val_auc'])\n",
    "\n",
    "        keras.backend.clear_session()\n",
    "        print(cross_validation)\n",
    "        print(np.array(cross_validation).mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae411b7-534e-4d90-8b71-6c906951db9c",
   "metadata": {},
   "source": [
    "# Analyzing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab6631-eed7-4fd9-8cca-617431749507",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_final_test = '../models/20210908-160537_conv_gender/fold_6_final_8_val.csv'\n",
    "# path_to_final_test = '../models/20210908-201527_bigru_gender_final/fold_6_final_8_val.csv'\n",
    "# path_to_final_test = '../models/20210908-191345_conv_gender_final/fold_6_final_8_val.csv'\n",
    "# path_to_final_test = '../models/20210908-194758_dense_gender_final/fold_6_final_8_val.csv'\n",
    "# path_to_final_test = '../models/20210908-214419_gru_gender_final/fold_6_final_8_val.csv'\n",
    "# path_to_final_test = '../models/20210908-225146_lstm_gender_final/fold_6_final_8_val.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad287f4b-543f-4cca-85cd-1d85a1db4507",
   "metadata": {},
   "source": [
    "## Looking at false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0449ba1-e409-4718-894b-080f994ad613",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = pd.read_csv(path_to_final_test)\n",
    "final_test.head()\n",
    "over_threshold = final_test[final_test['prediction'] >= .84]\n",
    "false_positives = over_threshold[over_threshold['actual'] == 0]\n",
    "print(len(false_positives))\n",
    "# false_positives.head()\n",
    "false_positives.sort_values(by=['prediction'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c7e2ac-c3af-4836-8324-dd6b005f1bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_id = list(false_positives['patient'])\n",
    "patient_strings = []\n",
    "for id in list_id:\n",
    "    index = nn_patient_list.index(str(id))\n",
    "    patient_strings.append(nn_string_list[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48094108-ce9e-43e0-93e0-318575d4c0b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To display the id and terms of each patient\n",
    "# for patient_string, patient_id in zip(patient_strings, list_id):\n",
    "#     print(patient_id)\n",
    "#     print(patient_string.split('lineend'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032a3f27-a9e6-41e5-b5c5-539385221dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "of_interest = ['charge', 'alagille', 'williams', 'kabuki', 'syndrome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81706184-f00d-41f8-8211-b350b633906c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "contains_syndrome = []\n",
    "for patient_string in patient_strings:\n",
    "    list_form = patient_string.split('lineend')\n",
    "    relevant_terms = []\n",
    "    for item in list_form:\n",
    "        if any(x in item.lower() for x in of_interest):\n",
    "            relevant_terms.append(item)\n",
    "    contains_syndrome.append(relevant_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715ad9a0-4c2e-4f0f-875d-2c2cddf6e604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for syndrome_string, patient_id in zip(contains_syndrome, list_id):\n",
    "    if syndrome_string != []:\n",
    "        print(patient_id)\n",
    "        print(syndrome_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6b313-f27d-4a16-a5a2-04900819ab8d",
   "metadata": {},
   "source": [
    "## Checking the predictive value of individual terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f2d278-6f25-4f19-aac4-b077f3b16fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dx(model_paths, token_path, training_length=1000):\n",
    "    \"\"\"\n",
    "    Loads model for testing and predictions on dx\n",
    "    Args:\n",
    "        model_paths - (list) List of model file paths\n",
    "        token_path - (string) Path to the token file\n",
    "        training_length- (int) Number of words to be used\n",
    "    Returns:\n",
    "        prediction - (np.ndarray) Predictions for each DX\n",
    "        string_list - (list) Strings used for each DX\n",
    "        int_list - (list) string_list tokenized\n",
    "    \"\"\"\n",
    "    prediction = None\n",
    "    attribute_names, string_list = get_dx('../data/noonan_r3.csv')\n",
    "    string_list = list(string_list)\n",
    "    string_list += ['Female lineend', 'Male lineend']\n",
    "    int_list, indx_words, word_indx, token_path = string_to_ints(string_list, token_path)\n",
    "    features = pad_sequences(int_list, maxlen=training_length, padding='post', truncating='post')\n",
    "    for model_path in model_paths:\n",
    "        # summarize model.\n",
    "        model = load_model(model_path)\n",
    "        # model.summary()\n",
    "        # make predictions\n",
    "        output = model.predict(features, verbose=0)\n",
    "        if prediction is None:\n",
    "            prediction = output\n",
    "        else:\n",
    "            prediction = np.append(prediction, output, axis=1)\n",
    "    prediction = np.average(prediction, axis=1)\n",
    "    return prediction, string_list, int_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1fafe-088d-4cd3-9cb2-f0c12e64c90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_final_model = '../models/20210908-160537_conv_gender/fold_6_final.h5'\n",
    "output_file = '../models/20210908-160537_conv_gender/dx_predictions.csv'\n",
    "model_paths = [path_to_final_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b7fffa-e8dd-44bb-a96e-ced121ce04c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts, dx_strings, ints = test_dx(model_paths, token_path)\n",
    "with open(output_file, 'w', newline='\\n', encoding=\"ISO-8859-1\") as csvfile:\n",
    "    record_writer = csv.writer(csvfile, delimiter=',')\n",
    "    attribute_names = ['dx', 'prediction', 'length of string']\n",
    "    record_writer.writerow(attribute_names)\n",
    "    for i  in range(len(dx_strings)):\n",
    "        row = [dx_strings[i], predicts[i], len(ints[i])]\n",
    "        record_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367ab914-c11b-49bd-9afa-29669cb98202",
   "metadata": {},
   "source": [
    "## Ranking the individual terms for each detected true positive case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba6f99-0aa3-4fcf-a688-9c039fb5b8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_final_model = '../models/20210908-160537_conv_gender/fold_6_final.h5'\n",
    "path_to_final_test = '../models/20210908-160537_conv_gender/fold_6_final_8_val.csv'\n",
    "final_test = pd.read_csv(path_to_final_test)\n",
    "final_test.head()\n",
    "over_threshold = final_test[final_test['prediction'] >= .84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dccb49-684b-411b-a8a4-634798de002c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_positives = over_threshold[over_threshold['actual'] == 1]\n",
    "print(len(true_positives))\n",
    "true_positives = true_positives.sort_values(by=['prediction'], ascending=False)\n",
    "patient_indices = []\n",
    "for id in true_positives['patient']:\n",
    "    patient_indices.append(n_patient_list.index(str(id)))\n",
    "detected_patients = n_array[patient_indices]\n",
    "detected_patients_ints = list(np.array(n_int_list, dtype=object)[patient_indices])\n",
    "detected_patients_features = pad_sequences(detected_patients_ints, maxlen=1000, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de26d08-2e96-4944-b6e8-cd704d9f84a3",
   "metadata": {},
   "source": [
    "#### Uncomment block below to check false postives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d070cc31-470e-402f-ae40-f3fff21a2c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# false_positives = over_threshold[over_threshold['actual'] == 0]\n",
    "# print(len(false_positives))\n",
    "# false_positives = false_positives.sort_values(by=['prediction'], ascending=False)\n",
    "# patient_indices = []\n",
    "# for id in false_positives['patient']:\n",
    "#     patient_indices.append(nn_patient_list.index(str(id)))\n",
    "# detected_patients = nn_array[patient_indices]\n",
    "# detected_patients_ints = list(np.array(nn_int_list, dtype=object)[patient_indices])\n",
    "# detected_patients_features = pad_sequences(detected_patients_ints, maxlen=1000, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2317ddc-1425-4420-9d05-5cb755c1407d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "string_list = []\n",
    "removed_list = []\n",
    "patient_id_list = []\n",
    "for patient in detected_patients:\n",
    "    patient_features = np.array(patient)\n",
    "    uniques = set(patient_features[:,3])\n",
    "    uniques = [\"None\"] + list(uniques)\n",
    "    patient_id = patient_features[0,0]\n",
    "    patient_id_list = patient_id_list + [patient_id] * len(uniques)\n",
    "    removed_list = removed_list + uniques\n",
    "    for item in uniques:\n",
    "        patient_dx_string = patient_features[0][4] + \" lineend \"\n",
    "        for patient_feature in patient_features:\n",
    "            dx_name = patient_feature[3]\n",
    "            if dx_name != item:\n",
    "                patient_dx_string = patient_dx_string + dx_name + \" lineend \"\n",
    "        string_list.append(patient_dx_string)\n",
    "int_list, _indx_words, _word_indx, _token_path = string_to_ints(string_list, token_path)\n",
    "new_features = pad_sequences(int_list, maxlen=1000, padding='post', truncating='post')\n",
    "\n",
    "new_prediction = None\n",
    "output = model.predict(new_features, verbose=0)\n",
    "new_prediction = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2166b093-6c9a-4d7f-8841-386ee576a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prediction_list = new_prediction.reshape(len(patient_id_list))\n",
    "differences = []\n",
    "for new_value, patient_id in zip(new_prediction_list, patient_id_list):\n",
    "    original_prediction = true_positives[true_positives['patient'] == int(patient_id)]['prediction'].values[0]\n",
    "    difference = round(new_value - original_prediction, 5)\n",
    "    differences.append(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfaaf48-6907-4c5d-80d7-eee69ba7a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_df = pd.DataFrame(list(zip(patient_id_list, removed_list, new_prediction_list, differences)), \n",
    "                          columns=['patient_id', 'term_removed', 'prediction_post_removal', 'difference_in_prediction_score'])\n",
    "removed_df = removed_df.sort_values(by=['patient_id', 'difference_in_prediction_score'], ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ca2866-f9d1-4b66-836e-7911ed7239f1",
   "metadata": {},
   "source": [
    "### Saves removed_df to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4528d9-10a0-4690-a1ae-46389dfed968",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_df.to_csv(index=False, path_or_buf='../models/20210908-160537_conv_gender/removed2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275a06c-c2bb-4ba1-9017-0a83e2f65f0e",
   "metadata": {},
   "source": [
    "## Biobank Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02003fc0-a234-4226-a4bb-16949e2a1fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_sort_data(path, num_examples=None):\n",
    "    \"\"\"\n",
    "    Sorts the data based on patient number\n",
    "    Args:\n",
    "        path - (string) Path to csv file\n",
    "        num_examples - (int) Number of samples to load\n",
    "    Returns:\n",
    "        features - (np.ndarray) numpy array containing all the information of each patient\n",
    "        attribute_names - (np.ndarray) numpy array containing the headers\n",
    "        string_list - (list) list of strings descriptions\n",
    "        patient_list - (list) list of patient ids\n",
    "    \"\"\"\n",
    "    attribute_names = []\n",
    "    data = []\n",
    "    string_list = []\n",
    "    patient_list = []\n",
    "    prev_id = None\n",
    "    patients_loaded = 0\n",
    "    with open(path, newline='\\n', encoding=\"ISO-8859-1\") as csvfile:\n",
    "        record_reader = csv.reader(csvfile, delimiter=',')\n",
    "        attribute_names = next(record_reader)\n",
    "        for row in record_reader:\n",
    "            row_id = row[0]\n",
    "            dx_name = row[3]\n",
    "            gender = row[5]\n",
    "            if prev_id == None:\n",
    "                new_patient = []\n",
    "                patient_dx_string = gender + \" lineend \"\n",
    "                patients_loaded += 1\n",
    "            elif prev_id != row_id:\n",
    "                if num_examples != None and patients_loaded >= num_examples:\n",
    "                    break\n",
    "                data.append(new_patient)\n",
    "                string_list.append(patient_dx_string)\n",
    "                patient_list.append(prev_id)\n",
    "                new_patient = []\n",
    "                patient_dx_string = gender + \" lineend \"\n",
    "                patients_loaded += 1\n",
    "            new_patient.append(row)\n",
    "            patient_dx_string = patient_dx_string + dx_name + \" lineend \"\n",
    "            prev_id = row_id\n",
    "        data.append(new_patient)\n",
    "        string_list.append(patient_dx_string)\n",
    "        patient_list.append(prev_id)\n",
    "    print('{file} read'.format(file=path))\n",
    "    return np.array(data, dtype=object), np.array(attribute_names), string_list, patient_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e2f4d4-452b-4f0d-9444-e039acdd58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = ['../models/final_model/fold_6_final.h5']\n",
    "patient_path = '../data/1275_BTM_non_noonan_r3.csv'\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f0c17-5bd6-464a-a511-5051f5e91f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict(model_paths, patient_path, token_path, training_length=1000):\n",
    "    prediction = None\n",
    "    # load dataset\n",
    "    patient_array, attribute_names, string_list, patient_list = custom_sort_data(patient_path)\n",
    "    int_list, indx_words, word_indx, token_path = string_to_ints(string_list, token_path)\n",
    "    features = pad_sequences(int_list, maxlen=training_length, padding='post', truncating='post')\n",
    "    for model_path in model_paths:\n",
    "        # summarize model.\n",
    "        model = load_model(model_path)\n",
    "        # model.summary()\n",
    "        # make predictions\n",
    "        output = model.predict(features, verbose=0)\n",
    "        if prediction is None:\n",
    "            prediction = output\n",
    "        else:\n",
    "            prediction = np.append(prediction, output, axis=1)\n",
    "        print(\"prediction done for %s \" %(model_path))\n",
    "    prediction = np.average(prediction, axis=1)\n",
    "    return prediction, patient_list, string_list, int_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb04adc2-d81a-492a-8bb2-0a9fa7669cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts, patients, strings, ints = make_predict(model_paths, patient_path, token_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b17f03-decb-4086-aa9e-b64e7038b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df = pd.DataFrame(list(zip(patients, predicts, strings, ints)), \n",
    "                          columns= ['patient_id', 'prediction', 'string', 'ints'])\n",
    "predicted_df = predicted_df.sort_values(by=['prediction'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c7974-87e6-4d5a-b180-3601e6cb422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "over_threshold = predicted_df[predicted_df['prediction'] >= .84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c74f3b-4fb3-4a0b-896c-537cc4103d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "over_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd27b64a-84ea-4c0a-b217-cf9bd539937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "of_interest = ['charge', 'alagille', 'williams', 'kabuki', 'syndrome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7365e32b-15d1-4f1e-8b81-a9f369c9428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "of_interest = ['syndrome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e729a-e437-44a3-a150-16032893587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_strings = list(over_threshold['string'])\n",
    "list_id = list(over_threshold['patient_id'])\n",
    "contains_syndrome = []\n",
    "for patient_string in patient_strings:\n",
    "    list_form = patient_string.split('lineend')\n",
    "    relevant_terms = []\n",
    "    for item in list_form:\n",
    "        if any(x in item.lower() for x in of_interest):\n",
    "            relevant_terms.append(item)\n",
    "    relevant_terms = set(relevant_terms)\n",
    "    contains_syndrome.append(relevant_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d0e31-b89f-48bb-a2f2-56f8bf5b8d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for syndrome_string, patient_id in zip(contains_syndrome, list_id):\n",
    "    if syndrome_string != set():\n",
    "        count += 1\n",
    "        print(patient_id)\n",
    "        print(syndrome_string)     \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302dd39-5117-4efd-b77f-4c6d5bb0f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '../models/final_model/patient_predictions.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d444258-0d2b-48c9-8072-6a65edfd6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# over_threshold_trimmed = over_threshold.drop('ints', 1)\n",
    "# over_threshold_trimmed.to_csv(index=False, path_or_buf=output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d030c-8408-48ff-8909-44dbcb7973ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df_trimmed = predicted_df.drop('string', 1)\n",
    "predicted_df_trimmed= predicted_df_trimmed.drop('ints', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff9c31a-14f5-4f12-9034-a222ea79568c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_df_trimmed.sort_values(by=['prediction'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc77ba4-57b0-454f-b7d5-4d651ea437a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df_trimmed.to_csv(index=False, path_or_buf=output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f85f996-bc4b-4ef0-b389-9f76c99fed6f",
   "metadata": {},
   "source": [
    "### Loading samples from ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09524c97-3711-4de2-8d13-0423e8ce4cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_path = '../data/pseudo_prospective_result.csv'\n",
    "id_df = pd.read_csv(id_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf91a5f-c0b5-42d0-ad3a-9c0e254d9ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "noonan_id_df = id_df[id_df[\"label\"] == 1]\n",
    "non_noonan_id_df = id_df[id_df[\"label\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f07ada1-6216-4492-8831-55d18b7ea1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_noonan_id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1273ed81-50ac-4abb-a4b6-0bd91e52bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features_list = list(n_features)\n",
    "nn_features_list = list(nn_features)\n",
    "n_patients_list = list(n_patient_array)\n",
    "nn_patients_list = list(nn_patient_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ddd3e8-55ac-402f-a29d-f7484e8ad23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "noonan_zipped = list(zip(n_patients_list, n_features_list))\n",
    "non_noonan_zipped = list(zip(nn_patients_list, nn_features_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1e798a-f9b2-4f86-b457-77bb0b45562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noonan = pd.DataFrame(noonan_zipped, columns=[\"pid\", \"sequence\"])\n",
    "df_non_noonan = pd.DataFrame(non_noonan_zipped, columns=[\"pid\", \"sequence\"])\n",
    "df_noonan['pid'] = pd.to_numeric(df_noonan[\"pid\"])\n",
    "df_non_noonan['pid'] = pd.to_numeric(df_non_noonan[\"pid\"])\n",
    "print(\"converted to ints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a277790b-679d-4641-aa37-efb5282517d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noonan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9afad8c-5435-40a9-968c-097f6173dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = id_df.merge(df_non_noonan, on=\"pid\")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452c0c23-a61c-4963-9cfb-875537f4c194",
   "metadata": {},
   "source": [
    "### import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649f4f00-7d01-4aeb-9593-4a97225315a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/20210908-160537_conv_gender/fold_6_final.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad9372-d8b6-4803-b837-5149bd8b8058",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_list = merged_df[\"sequence\"].to_list()\n",
    "labels = merged_df[\"label\"].to_list()\n",
    "patients = merged_df[\"pid\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58746f56-258c-4ac3-ac9e-2ec3710f91f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "padded_sequences = pad_sequences(int_list, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda6b6de-14cb-47e0-9c0f-759360358dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2197d8be-5234-4451-9644-33d73625fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604dd1a4-05aa-4b0e-bcd9-6dd80cdd5e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = calculate_pr_recall(output, labels, threshold=.01)\n",
    "analysisdir = \"../models/20210908-160537_conv_gender/prelim_pr.csv\"\n",
    "save_chart(analysis, analysisdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9f7518-19d5-4c58-a5f2-414c9939c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "valdir = \"../models/20210908-160537_conv_gender/prelim_val.csv\"\n",
    "with open(valdir, 'w', newline='\\n', encoding=\"ISO-8859-1\") as csvfile:\n",
    "    record_writer = csv.writer(csvfile, delimiter=',')\n",
    "    attribute_names = ['prediction', 'actual', 'patient']\n",
    "    record_writer.writerow(attribute_names)\n",
    "    for i  in range(len(output)):\n",
    "        row = [output[i][0], labels[i], patients[i]]\n",
    "        record_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ea1a35-0fac-41d8-b6e3-b0758a0d083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chartdir = \"../models/20210908-160537_conv_gender/prelim_pr.pdf\"\n",
    "plot_pr_recall(valdir, chartdir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
